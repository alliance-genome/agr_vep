# Alliance of Genome Resources VEP pipelines

A collection of pipelines for tasks associated with VEP annotation of variations.  The code contained within this repository is designed to run on the EBI's compute farm using the EnsEMBL Hive pipeline manager.

See the README files in each folder for details of the individual pipelines.


## VepProteinFunction

- Generates databases of SIFT and PolyPhen scores and predictions for the MOD species.  
- Databases are accessed by a VEP plugin to add SIFT/PolyPhen annotation to the VEP output.
- Translated protein sequences are constructed from FASTA, GFF, and (optionally) BAM files.
- Serialized prediction matrices are stored containing all possible amino acid substitutions for each sequence, accessed by the hex md5 of the sequence.
- FULL mode generates the database from scratch.
- UPDATE mode checks for existing prediction matrices, or sequences for which there were valid reasons for being unable to generate protein function annotations, and updates the database for new sequences only.


## ModVep

- Runs VEP on MOD high throughput variation VCF files
- Splits input files, runs VEP in parallel, then combines the output.
- Uses MOD GFF, FASTA, and (optionally) BAM files to construct translated protein sequences.
- Retrieves SIFT and PolyPhen annotations from databases generated by the VepProteinFunction pipeline.


## Running for AGR

The script `run_agr_vep_pipelines.pl` can be used to run both the pipelines above, in addition to running the VEP for phenotypic variants.

### Required software

- SIFT
- PolyPhen-2 (see README in VepProteinFunction directory for special setup instructions)
- Ensembl VEP
- Ensembl Hive
- bgzip
- tabix
- Perl
- BLAST
- BLAT

Perl libraries:

- File::Copy
- File::Path
- Data::Dumper
- Digest::MD5
- Bioperl

### Environmental variables

The following environmental variables need to be set:

- `AGR_VEP_BASE_DIR` - the directory in which results will be stored
- `AGR_VEP_REPO_DIR` - the path to the directory containing this repository
- `AGR_RELEASE` - the release number of the AGR release, e.g. 8.2.0
- `TOKEN` - the authorisation token used by AGR Data Quartermasters when submitting data to the FMS
- `DOWNLOAD_DATE` - the date on which the pipeline was initiated in the format YYYYMMDD
- `HIVE_ROOT_DIR` - path to clone of [Ensembl Hive GitHub resitory](https://github.com/Ensembl/ensembl-hive)
- `HTP_VEP_WORKING_DIR` - path to directory for intermediate working files created during running of ModVep pipeline
- `PATH_PRED_WORKING_DIR` - path to directory for intermediate working files created during running of VepProteinFunction pipeline
- `PATH_PRED_SQL_DUMP_DIR` - directory in which to store backup dumps of pathogenicity prediction SQL databases
- `PATH_PRED_DB_PREFIX` - prefix for MOD pathogenicity prediction SQL database names, e.g. agr_pathogenicity_predictions_
- `VEP_DIR` - path to clone of [Ensembl VEP GitHub resitory](https://github.com/Ensembl/ensembl-vep)
- `VEP_DBHOST` - host name of MySQL server with eHive and pathogenicity prediction databases
- `VEP_DBUSER` - username for MySQL server with eHive and pathogenicity prediction databases
- `VEP_DBPORT` - port number of MySQL server with eHive and pathogenicity prediction databases
- `SIFT_DIR` - directory containing SIFT installation
- `PPH_DIR` - directory containing PolyPhen-2 installation
- `PPH_CONF_DIR` - path to `VepProteinFunction/polyphen_config_files` folder in this repository
- `NCBI_DIR` - path to directory where psiblast and makeblastdb are installed
- `BLAST_DB` - path to directory containing UniRef90 BLAST database files
- `PPH_BLAST_DB` - path to directory containing UniRef100 BLAST database files
- `UNIPROT_DBS` - path to directory containing uniprot species-specific BLAST protein database files
- `TMP_ROOT_DIR` - path to a folder for temporary storage, preferentially with fast read-write access

### Running

To run the VEP analyses and update pathogenicity prediction score for the Alliance:

   perl $CVS_DIR/AGR/run_agr_vep_pipelines.pl -mod FB,MGI,RGD,SGD,WB,ZFIN,HUMAN -stages 1,2,3,4,5 -password [mysql_password] -url [agr_fms_snapshot_url]

The `-mod` and `-stages` parameters should include only the MODs you want to run the VEP for, and only the stages you want to run (as outlined below).  The `-url` parameter only needs to be specified if you want to download the files from a specific FMS snapshot, rather than using the latest files.

#### Stage 1

Downloads AGR files from the following FMS buckets: FASTA, GFF, VCF, HTVCF, MOD-GFF-BAM-KNOWN, MOD-GFF-BAM-MODEL.

Files are saved in: `${AGR_VEP_BASE_DIR}/${AGR_RELEASE}/${DOWNLOAD_DATE}/` in subfolders with the MOD name

Renames the downloaded files.  If more than one BAM file is downloaded (ie. a file corresponding to known transcripts and a file corresponding to model transcripts), the BAM files are merged into a single file.

If you want to skip stage 1 because it has already been carried out you need to ensure that the files are available in the folders described above, that `$DOWNLOAD_DATE` is set appropriately, and that the filenames are in the following format (not all filetypes will necessarily be present for all MODs):
- MOD_FASTA.fa
- MOD_GFF.gff
- MOD_VCF.vcf
- MOD_HTVCF.vcf
- MOD_BAM.bam

#### Stage 2

Processes the GFF and FASTA files for use with the VEP. 

FASTA files are compressed with bgzip.  GFF files are parsed to correct for known issues with using the MOD submitted GFFs and the VEP (largely the addition of biotype tags to the attributes column), then compressed using bgzip, and indexed using tabix.

#### Stage 3

Updates the databases of SIFT and PolyPhen scores after creating backups of the existing databases.

This starts an eHive pipeline.  The pathogenicity prediction score databases are named agr_pathogenicity_predictions_MOD, while the corresponding eHive databases are named agr_pathogenicity_predictions_mod_ehive.

Only translation sequences which have not previously been processed by the pipeline should be analysed by SIFT and/or PolyPhen.  Particularly large numbers of schedule jobs (i.e. 1,000s) is indicative of an issues (possibly with the MOD-submitted files).  You can expect a high job failure rate as the pipeline will attempt to reanalyse translation sequences that failed in previous runs, although there is a maximum number of reattempts to prevent the accumulation of jobs destined to fail.

If you need a quick turnaround then this stage can be skipped.        

#### Stage 4

Runs the VEP on the phenotypic variations (i.e. those in the MOD_VCF.vcf files), and uploads the resulting files to the AGR FMS.

Carries out VEP runs at the transcript and gene level, resulting in MOD_VEPTRANSCRIPT.txt and MOD_VEPGENE.txt files being generated and uploaded to the VEPGENE and VEPTRANSCRIPT FMS buckets.

If you run the script with the `-test` flag then the files will not be uploaded to AGR.

#### Stage 5

Runs the VEP on the high throughput variations (i.e. those in the MOD_HTVCF.vcf files), and uploads the resulting files to the AGR FMS.

The eHive databases for the pipelines started during this stage are named agr_htp_mod_vep_ehive.  The resulting files are named MOD_HTPOSTVEPVCF.vcf.gz and are uploaded to the HTPOSTVEP FMS bucket (unless the script is run with the -test flag).

#### Notes

If there are any issues with the eHive pipelines run during stages 3 or 5 then you can find the log messages in the log_message table of the eHive databases.

The eHive databases are named agr_pathogenicity_predictions_mod_ehive and agr_htp_mod_vep_ehive (with the MOD name in lower case).

If uploading manually (e.g. because you ran the above with the test flag) the the upload commands are in the following format, replacing RGD with the appropriate MOD abbreviation:  

   curl -H "Authorization: Bearer ${TOKEN}" -X POST "https://fms.alliancegenome.org/api/data/submit" -F "${AGR_RELEASE}_HTPOSTVEPVCF_RGD=@RGD_HTPOSTVEPVCF.vcf.gz"

For high-throughput variants, only the RGD files gets uploaded to the HTPOSTVEPVCF bucket.  All VEP annotated VCF files produced by this pipeline need to be uploaded to the appropriate s3 bucket for each MOD, e.g.:

    aws s3 sync ${AGR_VEP_BASE_DIR}/${AGR_RELEASE}/${DOWNLOAD_DATE}/RGD/HTPVEP s3://mod-datadumps/variants/${AGR_RELEASE}/RGD

Periodically, the input files for the high-throughput variants need to be updated (note: this has traditionally been done by whoever is running the VEP pipeline rather than Data Quartermasters).  With the exception of WormBase (who generate their high-throughput variants file each release), VCF files of the high throughput variants need to be submitted to the HTVCF FMS bucket.  In the case of humans, we currently only submit variants that are in the ClinVar set.  For those MODs that use RefSeq transcripts (currently HUMAN / RGD / MGI / ZFIN) there is an additional requirment for BAM files to be submitted to the MOD-GFF-BAM-MODEL and MOD-GFF-BAM-KNOWN FMS buckets in order to correct for differences between the RefSeq transcript sequences and the reference genome assembly.  These BAM files are available from the [RefSeq FTP site](https://ftp.ncbi.nlm.nih.gov/refseq/).